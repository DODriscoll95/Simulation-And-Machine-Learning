{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from mpl_toolkits import mplot3d\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "plt.rcParams[\"figure.figsize\"] = [7,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following project we will examine a German Credit Risk dataset. This dataset has 1000 entries with each entry representing an individual seeking credit. Each individual has been classified as a good(+1) or bad credit(-1) risk according to a set of features. Our goal here is to train a Support Vector Machine Classifier on the dataset to predict Credit Risk\n",
    "\n",
    "In this notebook we will first import and prepare the data to be used in the SVMs. We will then investigate different tpyes of SVMs and different parameters to be used, and will then choose a set of parameters offering the best level of correct predicition of credit. We will then run these classifiers on test data to see how well our final classifier performs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ger = pd.read_csv(\"german.data-numeric-withheader.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking data read in correctly\n",
    "ger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#irst 5 rows\n",
    "ger.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting design matrix X and labels y\n",
    "X = ger.iloc[:,1:]\n",
    "y = ger[\"CREDITRATING\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking shapes\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting historgrams of a few features\n",
    "\n",
    "fig, ax = plt.subplots(2,2)\n",
    "ax[0,0].hist(X[\"Loan_NurnMonth\"])\n",
    "ax[0,0].set_title('Loan Duration Months')\n",
    "ax[0,1].hist(X[\"CreditAmt\"])\n",
    "ax[0,1].set_title('Credit Amount')\n",
    "ax[1,0].hist(X[\"AgeInYears\"])\n",
    "ax[1,0].set_title('Age')\n",
    "ax[1,1].hist(X[\"ForeignWorker\"])\n",
    "ax[1,1].set_title('Foriegn Worker Status')\n",
    "fig.suptitle('Histograms of certain features')\n",
    "plt.show()\n",
    "plt.rcParams[\"figure.figsize\"] = [7,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loan Duration in Years historgram\n",
    "LoanDurYr = X[\"Loan_NurnMonth\"]/12\n",
    "plt.hist(LoanDurYr)\n",
    "plt.title(\"Loan Duration in Years\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating train test split using 25% (the default value) of data as test data.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.25, random_state = 7 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation Score with no scaling\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"The mean classifier score is\", clf.score(X_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling Using Standard Scaler\n",
    "#setting our scaler\n",
    "scaler_std = preprocessing.StandardScaler()\n",
    "\n",
    "#fitting scaler to training data\n",
    "scaler_std.fit(X_train)\n",
    "X_train_scaled_std = scaler_std.transform(X_train)\n",
    "\n",
    "#applying scaler to test data\n",
    "X_test_scaled_std = scaler_std.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeat Scaling Using MinMaxScaler\n",
    "\n",
    "scaler_mm = preprocessing.MinMaxScaler()\n",
    "scaler_mm.fit(X_train)\n",
    "X_train_scaled_mm = scaler_mm.transform(X_train)\n",
    "X_test_scaled_mm = scaler_mm.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing different kernels and different scaling techniques\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Linear\n",
    "\n",
    "clf = svm.SVC(kernel = 'linear')\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf,X_train,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for linear is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "#poly\n",
    "clf = svm.SVC(kernel = 'poly')\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf,X_train,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for poly is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "#rbf\n",
    "clf = svm.SVC(kernel = 'rbf')\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf,X_train,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for rbf is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "#sigmoid\n",
    "clf = svm.SVC(kernel = 'sigmoid')\n",
    "clf.fit(X_train, y_train)\n",
    "scores = cross_val_score(clf,X_train,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for sigmoid is\", '{0:.4g}'.format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Scaling\n",
    "\n",
    "clf = svm.SVC(kernel = 'linear')\n",
    "clf.fit(X_train_scaled_std, y_train)\n",
    "scores = cross_val_score(clf,X_train_scaled_std,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for linear is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "clf = svm.SVC(kernel = 'poly')\n",
    "clf.fit(X_train_scaled_std, y_train)\n",
    "scores = cross_val_score(clf,X_train_scaled_std,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for poly is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "clf = svm.SVC(kernel = 'rbf')\n",
    "clf.fit(X_train_scaled_std, y_train)\n",
    "scores = cross_val_score(clf,X_train_scaled_std,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for rbf is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "clf = svm.SVC(kernel = 'sigmoid')\n",
    "clf.fit(X_train_scaled_std, y_train)\n",
    "scores = cross_val_score(clf,X_train_scaled_std,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for sigmoid is\", '{0:.4g}'.format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMaxScaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "clf = svm.SVC(kernel = 'linear')\n",
    "clf.fit(X_train_scaled_mm, y_train)\n",
    "scores = cross_val_score(clf,X_train_scaled_mm,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for linear is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "clf = svm.SVC(kernel = 'poly')\n",
    "clf.fit(X_train_scaled_mm, y_train)\n",
    "scores = cross_val_score(clf,X_train_scaled_mm,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for poly is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "clf = svm.SVC(kernel = 'rbf')\n",
    "clf.fit(X_train_scaled_mm, y_train)\n",
    "scores = cross_val_score(clf,X_train_scaled_mm,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for rbf is\", '{0:.4g}'.format(mean))\n",
    "\n",
    "clf = svm.SVC(kernel = 'sigmoid')\n",
    "clf.fit(X_train_scaled_mm, y_train)\n",
    "scores = cross_val_score(clf,X_train_scaled_mm,y_train)\n",
    "mean = scores.mean()\n",
    "print(\"The mean classifier score for sigmoid is\", '{0:.4g}'.format(mean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its clear that both methods of scaling data overall performed better than the unscaled data.\n",
    "\n",
    "And RBF and linear seem to perform the best purely based off of mean classifier scores so we will only consider those moving forward.\n",
    "\n",
    "We will move forward using the MinMax scaling. Although it did perform marginally worse than StandardScaling for rbf kernel, the utilisation of standard scaling has the possibilty of not performing as well as we would like to if the underlying data isnt normally distributed data, which may not be true from this dataset. Although it performed fine for us here, if one of the features has an extremely large variance, it may affect our ability of the estimator to learn from other features correctly.\n",
    "\n",
    "The motivation to minmax scaling include robustness to very small standard deviations of features and preserving zero entries in sparse data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled_mm = scaler_mm.transform(X_train)\n",
    "X_test_scaled_mm = scaler_mm.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning for Linear kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter tuning for linear\n",
    "\n",
    "# set the grid of cavalues\n",
    "C_vals = [0.001, 0.005, 0.01,0.05,0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000]\n",
    "#creating emptpy arrays for crossval scores\n",
    "\n",
    "Ncases = len(C_vals)\n",
    "score_mean = np.zeros(Ncases)\n",
    "score_std = np.zeros(Ncases)\n",
    "for k in range(Ncases):\n",
    "    # set the classifier with the corresponding hyperparameter\n",
    "    clf = svm.SVC(kernel = 'linear', C = C_vals[k])\n",
    "    # This the cross-validation and the important and expensive part of the code.\n",
    "    scores = cross_val_score(clf,X_train_scaled_mm,y_train)  \n",
    "    # record the mean and std of the score\n",
    "    score_mean[k] = scores.mean()\n",
    "    score_std[k] = scores.std()\n",
    "    print(\"C = \",  (C_vals[k]), \", Avg Score = \",'{0:.4g}'.format(score_mean[k]))\n",
    "    \n",
    "# plot the scores as function of hyperparameter\n",
    "plt.semilogx(C_vals,score_mean,'r',label = 'Cross Val Score')\n",
    "plt.fill_between(C_vals,score_mean-score_std,score_mean+score_std,alpha=0.2,label = 'Score +/- std')\n",
    "plt.xlabel(\"C\", fontsize=\"14\")\n",
    "plt.ylabel(\"mean score +/- std\", fontsize=\"14\")\n",
    "plt.title('Cross Validation Scores for varying values of C, linear kernel')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We note how the scores for 0.1,0.5,1 are essentially the same, but the 95% CI for C = 1 is slightly smaller, hence we will move forward using C = 1 when using our kernel as linear\n",
    "\n",
    "### Tuning for rbf kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter C tuning for rbf\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# set the grid of C vals\n",
    "C_vals = [0.001, 0.005, 0.01,0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000]\n",
    "Ncases = len(C_vals)\n",
    "score_mean = np.zeros(Ncases)\n",
    "score_std = np.zeros(Ncases)\n",
    "\n",
    "for k in range(Ncases):\n",
    "    # set the classifier with the corresponding hyperparameter\n",
    "    clf = svm.SVC(kernel = 'rbf', C = C_vals[k])\n",
    "    # This the cross-validation. It is the important and expensive part of the code.\n",
    "    scores = cross_val_score(clf,X_train_scaled_mm,y_train)  \n",
    "    # record the mean and std of the score\n",
    "    score_mean[k] = scores.mean()\n",
    "    score_std[k] = scores.std()\n",
    "    print(\"C = \",  (C_vals[k]), \", Avg Score = \",'{0:.4g}'.format(score_mean[k]))\n",
    "    \n",
    "# plot the scores as function of hyperparameter\n",
    "plt.semilogx(C_vals,score_mean,'r',label = 'Cross Val Score')\n",
    "plt.fill_between(C_vals,score_mean-score_std,score_mean+score_std,alpha=0.2,label = 'Score +/- std')\n",
    "plt.xlabel(\"C\", fontsize=\"14\")\n",
    "plt.ylabel(\"mean score +/- std\", fontsize=\"14\")\n",
    "plt.title('Cross Validation Scores for varying values of C, rbf kernel')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter gamma tuning for rbf\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# set the grid of gamma vals\n",
    "gam_vals = [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 10, 50, 100, 500, 1000]\n",
    "Ncases = len(gam_vals)\n",
    "score_mean = np.zeros(Ncases)\n",
    "score_std = np.zeros(Ncases)\n",
    "\n",
    "for k in range(Ncases):\n",
    "    # set the classifier with the corresponding hyperparameter\n",
    "    clf = svm.SVC(kernel = 'rbf', C = 1, gamma = gam_vals[k])\n",
    "    # This the cross-validation. It is the important and expensive part of the code.\n",
    "    scores = cross_val_score(clf,X_train_scaled_mm,y_train)  \n",
    "    # record the mean and std of the score\n",
    "    score_mean[k] = scores.mean()\n",
    "    score_std[k] = scores.std()\n",
    "    print(\"Gamma = \",  (gam_vals[k]), \", Avg Score = \",'{0:.4g}'.format(score_mean[k]))\n",
    "print('\\n')\n",
    "\n",
    "# plot the scores as function of hyperparameter\n",
    "plt.semilogx(gam_vals,score_mean,'r',label = 'Cross Val Score')\n",
    "plt.fill_between(gam_vals,score_mean-score_std,score_mean+score_std,alpha=0.2,label = 'Score +/- std')\n",
    "plt.xlabel(\"Gamma\", fontsize=\"14\")\n",
    "plt.ylabel(\"mean score +/- std\", fontsize=\"14\")\n",
    "plt.title('Cross Validation Scores for varying values of gamma, rbf kernel')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these two tests it seems the average score is maximised when C is one of our values between 0.5 and 10, and gamma is either 0.1 or 1\n",
    "Further testing for these parameters is below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vals = [0.5,1,5,10]\n",
    "Ncases = len(C_vals)\n",
    "score_mean = np.zeros(Ncases)\n",
    "score_std = np.zeros(Ncases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing gamma as 0.1\n",
    "for k in range(Ncases):\n",
    "    # set the classifier with the corresponding hyperparameter\n",
    "    clf = svm.SVC(kernel = 'rbf', C = C_vals[k], gamma = 0.1)\n",
    "    # This the cross-validation. It is the important and expensive part of the code.\n",
    "    scores = cross_val_score(clf,X_train_scaled_mm,y_train)  \n",
    "    # record the mean and std of the score\n",
    "    score_mean[k] = scores.mean()\n",
    "    score_std[k] = scores.std()\n",
    "    print(\"C = \",  (C_vals[k]), \", Avg Score = \",'{0:.4g}'.format(score_mean[k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixing gamma as 1\n",
    "for k in range(Ncases):\n",
    "    # set the classifier with the corresponding hyperparameter\n",
    "    clf = svm.SVC(kernel = 'rbf', C = C_vals[k], gamma = 1)\n",
    "    # This the cross-validation. It is the important and expensive part of the code.\n",
    "    scores = cross_val_score(clf,X_train_scaled_mm,y_train)  \n",
    "    # record the mean and std of the score\n",
    "    score_mean[k] = scores.mean()\n",
    "    score_std[k] = scores.std()\n",
    "    print(\"C = \",  (C_vals[k]), \", Avg Score = \",'{0:.4g}'.format(score_mean[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence just basing our decision off of mean cross validation scores we will set C = 5 and gamma = 0.1 for the rbf kernel, however it is possible that this pair of parameters may have higher variance than another pair of parameters that may have a slightly lower CV score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing and comparing classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default SVC classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train_scaled_mm, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test_scaled_mm)\n",
    "print(\"The classifier score is\", clf.score(X_test_scaled_mm, y_test))\n",
    "\n",
    "print(\"\\nThe confusion matrix is\")\n",
    "#Plot confusion matix\n",
    "plot_confusion_matrix(clf, X_test_scaled_mm, y_test)\n",
    "plt.title('confusion matrix for Default SVC')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\",\"Classification Report:\",\"\\n\",classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned Linear classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned SVC classifier with linear kernel, C = 1\n",
    "clf = svm.SVC(kernel = 'linear', C = 1)\n",
    "clf.fit(X_train_scaled_mm, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test_scaled_mm)\n",
    "print(\"The classifier score is\", clf.score(X_test_scaled_mm, y_test),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "plot_confusion_matrix(clf, X_test_scaled_mm, y_test)\n",
    "plt.title('confusion matrix for tuned linear SVC')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\",\"Classification Report:\",\"\\n\",classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuned Rbf Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tuned SVC classifier with rbf kernel, C = 5, gamma = 0.1\n",
    "clf = svm.SVC(kernel = 'rbf', C = 5, gamma = 0.1)\n",
    "clf.fit(X_train_scaled_mm, y_train)\n",
    "\n",
    "y_predict = clf.predict(X_test_scaled_mm)\n",
    "print(\"The classifier score is\", clf.score(X_test_scaled_mm, y_test),\"\\n\")\n",
    "\n",
    "\n",
    "plot_confusion_matrix(clf, X_test_scaled_mm, y_test)\n",
    "plt.title('confusion matrix for tuned rbf SVC')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\",\"Classification Report:\",\"\\n\",classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=7)\n",
    "tree.fit(X_train_scaled_mm,y_train)\n",
    "\n",
    "y_predict = tree.predict(X_test_scaled_mm)\n",
    "\n",
    "print(\"The classifier score is\", tree.score(X_test_scaled_mm, y_test),\"\\n\")\n",
    "plot_confusion_matrix(tree, X_test_scaled_mm, y_test)\n",
    "plt.title('Confusion Matrix for Decision Tree Classifier')\n",
    "plt.show()\n",
    "print(\"\\n\",\"Classification Report:\",\"\\n\",classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = MLPClassifier(learning_rate_init=0.01, max_iter=5000)\n",
    "regr.fit(X_train_scaled_mm, y_train)\n",
    "\n",
    "y_predict = regr.predict(X_test_scaled_mm)\n",
    "\n",
    "print(\"The classifier score is\", regr.score(X_test_scaled_mm, y_test),\"\\n\")\n",
    "\n",
    "\n",
    "plot_confusion_matrix(regr, X_test_scaled_mm, y_test)\n",
    "plt.title('Confusion Matrix for MLP Classifier')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\",\"Classification Report:\",\"\\n\",classification_report(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note how the default classifier performs pretty well, however we can improve upon it thanks to our hyper parameter tuning which shows how important carrying our the hyperparameter search is. The default decision tree performs the worst, but we have not tuned any of the parameters for the decision tree so it is possible we could improve its prediction power. The MLP Classifer performs slightly worse than the default SVC classifier.\n",
    "\n",
    "Finally after our hyper parameter search we were able to predict credit ratings based off of our training data with ~80% accuracy using SVM and hyperparameter tuning. This could potentially be improved by increasing the size of our training data if we were given a bigger overall sample size. This test study could also be improved by performing a grid search for the hyperparametrs but may take significantly more time and be more computer intensive. We could also exmaine the underlying data before carrying out the study and correct for outliers or see if the data follows any underlying distributions which may affect our choices in data scaling or choosing another method not mentioned here.\n",
    "\n",
    "We also note how in general all of the different classifiers perform better at correctly classifying someone who has bad credit in reality, into the bad credit category, given info from the 24 features. This isn't too surprising as there are 700 people with bad credit in the dataset and 300 with good credit so we will logically have a much smaller training and test sample for people with good credit ratings. So if we had a data set with equal number of people with good and bad credit would possibly help us improve our algorithm's accuracy. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
