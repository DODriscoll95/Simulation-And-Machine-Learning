{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import plot_partial_dependence\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will be training a multi-layer regressor on the calfornia housing prices dataset. This is a large dataset with over 20,000 samples. The data comes from the 1990 California census and\n",
    "summarises housing data by geographical region. We will then see how well our regressor performs on predicting housing prices on an out of training sample.\n",
    "\n",
    "Being able to carry out a task like this could be important for those looking at potential development projects and the estimated costs or types of houses certain cohorts of people may be interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = sklearn.datasets.fetch_california_housing( data_home=None, download_if_missing=True, return_X_y=False, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = a.data\n",
    "y = a.target\n",
    "y = y.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First few lines of data\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histograms of all data features\n",
    "h = X.hist(bins = 10, figsize = (15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating heatmap of correlation between housing features\n",
    "corr = X.corr()\n",
    "ax = plt.axes()\n",
    "sns.heatmap(corr, linewidths = 0.5,ax = ax, cmap = 'Spectral')\n",
    "ax.set_title('Heat map of correlation between data features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hist of median housing prices\n",
    "plt.hist(y, bins = 30)\n",
    "plt.title('Historgram of Median House Prices')\n",
    "plt.xlabel('Housing price in 100k')\n",
    "plt.ylabel('# of houses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating traintest split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling feature data using Quantile transformer\n",
    "quantile_quantformer = QuantileTransformer(n_quantiles=1000)\n",
    "X_train_quant = quantile_quantformer.fit_transform(X_train)\n",
    "X_test_quant = quantile_quantformer.transform(X_test)\n",
    "np.mean(X_test_quant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reshaping to avoid warnings\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "#Transforming target data to zero mean\n",
    "\n",
    "y_train_scale= y_train - np.mean(y_train)\n",
    "y_test_scale= y_test - np.mean(y_test)\n",
    "\n",
    "\n",
    "#Reshape back to avoid warnings\n",
    "y_train = y_train.ravel()\n",
    "y_test = y_test.ravel()\n",
    "y_train_scale = y_train_scale.ravel()\n",
    "y_test_scale = y_test_scale.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity Check.\n",
    "regr = MLPRegressor(max_iter=5000, learning_rate_init=0.01,random_state = 7)\n",
    "scores = cross_val_score(regr,X_train_quant,y_train_scale)  \n",
    "print(\"The regressor score for scaled data is\", '{0:.4g}'.format(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Easy way to set hidden layer values for powers of two and ten, can be ignored ##\n",
    "def Twos(n):\n",
    "    myList = []\n",
    "    for j in range(3, n+1):\n",
    "        number = 2 ** j\n",
    "        myList.append(number)\n",
    "    return myList\n",
    "\n",
    "def alphs(n):\n",
    "    myList = []\n",
    "    for j in range(-5, n+1):\n",
    "        number = 10 ** j\n",
    "        myList.append(number)\n",
    "    return myList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A single hidden layer with N = 2^3, 2^4,....., 2^7 neurons with the default ’relu’ activation function.\n",
    "\n",
    "Sizes = Twos(7)\n",
    "\n",
    "Ncases = len(Sizes)\n",
    "score_mean = np.zeros(Ncases)\n",
    "score_std = np.zeros(Ncases)\n",
    "looptimes = np.zeros(Ncases)\n",
    "\n",
    "#using loop to train the regressor and then run CV on the regressor and calculate the mean and std of the score\n",
    "for k in range(Ncases):\n",
    "    starttime = time.time()\n",
    "    regr = MLPRegressor(hidden_layer_sizes = (Sizes[k],), random_state=7, activation='relu', \n",
    "                        learning_rate_init=0.01, max_iter=5000)\n",
    "    # This the cross-validation. It is the important and expensive part of the code.\n",
    "    scores = cross_val_score(regr,X_train_quant,y_train_scale)  \n",
    "    # record the mean and std of the score\n",
    "    score_mean[k] = scores.mean()\n",
    "    score_std[k] = scores.std()\n",
    "    endtime = time.time()\n",
    "    looptimes[k] = endtime - starttime\n",
    "    print(\"Number of Neurons = \",  (Sizes[k]), \", Avg Score = \",'{0:.4g}'.format(score_mean[k]), \n",
    "          ', Time taken for this case is', '{0:.4g}'.format(looptimes[k]))\n",
    "    \n",
    "totaltime = sum(looptimes)    \n",
    "\n",
    "# plot the scores as function of hyperparameter\n",
    "\n",
    "plt.plot(Sizes,score_mean,'r',label = 'Cross Val Score')\n",
    "plt.fill_between(Sizes,score_mean-score_std,score_mean+score_std,alpha=0.2,label = 'Score +/- std')\n",
    "plt.xlabel(\"N\", fontsize=14)\n",
    "plt.ylabel(\"mean score +/- std\", fontsize=14)\n",
    "plt.title(\"Plot of mean scores for varying neuron sizes using Relu Act Func\")\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "print(\"Total training time = \", '{0:.4g}'.format(totaltime), \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "We will now repeat this for a single hidden layer with N = 2^3, 2^4, 2^5, neurons with the ’logistic’ activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Sizes = Twos(5)\n",
    "Ncases = len(Sizes)\n",
    "score_mean = np.zeros(Ncases)\n",
    "score_std = np.zeros(Ncases)\n",
    "looptimes = np.zeros(Ncases)\n",
    "for k in range(Ncases):\n",
    "    starttime = time.time()\n",
    "    regr = MLPRegressor(hidden_layer_sizes = (Sizes[k],), random_state=7, activation='logistic', \n",
    "                        learning_rate_init=0.01, max_iter=5000)\n",
    "    # This the cross-validation. It is the important and expensive part of the code.\n",
    "    scores = cross_val_score(regr,X_train_quant,y_train_scale)  \n",
    "    # record the mean and std of the score\n",
    "    score_mean[k] = scores.mean()\n",
    "    score_std[k] = scores.std()\n",
    "    endtime = time.time()\n",
    "    looptimes[k] = endtime - starttime\n",
    "    print(\"Number of Neurons = \",  (Sizes[k]), \", Avg Score = \",'{0:.4g}'.format(score_mean[k]), \n",
    "          ', Time taken for this case is', '{0:.4g}'.format(looptimes[k]))\n",
    "totaltime = sum(looptimes)\n",
    "# plot the scores as function of hyperparameter\n",
    "plt.plot(Sizes,score_mean,'r',label = 'Cross Val Score')\n",
    "plt.fill_between(Sizes,score_mean-score_std,score_mean+score_std,alpha=0.2,label = 'Score +/- Std')\n",
    "plt.xlabel(\"N\", fontsize=14)\n",
    "plt.ylabel(\"mean score +/- std\", fontsize=14)\n",
    "plt.title(\"Plot of mean scores for varying neuron sizes using Logistic Act Func\")\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "print(\"Total training time = \", '{0:.4g}'.format(totaltime), \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "A single hidden layer with N = 32 neurons with ’relu’ activation function, with the regularisation parameter\n",
    "\n",
    "Alpha = 10^-5, 10^-4,....10^-1\n",
    "\n",
    "Where alpha is an L2 penalty (regularisation term) parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A single hidden layer with N = 32 neurons with ’relu’ activation function, with the regularisation parameter\n",
    "#alpha = 10^-5, 10^-4,....10^-1\n",
    "\n",
    "alphas = alphs(-1)\n",
    "Ncases = len(alphas)\n",
    "score_mean = np.zeros(Ncases)\n",
    "score_std = np.zeros(Ncases)\n",
    "looptimes = np.zeros(Ncases)\n",
    "for k in range(Ncases):\n",
    "    starttime = time.time()\n",
    "    regr = MLPRegressor(alpha=alphas[k],hidden_layer_sizes = (32,), random_state=7, activation='relu', \n",
    "                        learning_rate_init=0.01, max_iter=5000)\n",
    "    # This the cross-validation. It is the important and expensive part of the code.\n",
    "    scores = cross_val_score(regr,X_train_quant,y_train_scale)  \n",
    "    # record the mean and std of the score\n",
    "    score_mean[k] = scores.mean()\n",
    "    score_std[k] = scores.std()\n",
    "    endtime = time.time()\n",
    "    looptimes[k] = endtime - starttime\n",
    "    print(\"Alpha = \",  (alphas[k]), \", Avg Score = \",'{0:.4g}'.format(score_mean[k]), \n",
    "          ', Time taken for this case is', '{0:.4g}'.format(looptimes[k]))\n",
    "    \n",
    "totaltime = sum(looptimes)\n",
    "\n",
    "# plot the scores as function of hyperparameter\n",
    "plt.semilogx(alphas,score_mean,'r',label = 'Cross Val Score')\n",
    "plt.fill_between(alphas,score_mean-score_std,score_mean+score_std,alpha=0.2,label = 'Score +/- std')\n",
    "plt.xlabel(\"N\", fontsize=14)\n",
    "plt.ylabel(\"mean score +/- std\", fontsize=14)\n",
    "plt.title(\"Plot of mean scores for varying alphas Func\")\n",
    "plt.legend()\n",
    "plt.show()    \n",
    "print(\"Total training time = \", '{0:.4g}'.format(totaltime), \" seconds\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "Multiple hidden layers hidden layer sizes = 32, (32,32,32), and (32,32,32,32,32) with the\n",
    "relu activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple hidden layers hidden layer sizes = 32, (32,32,32), and (32,32,32,32,32) with the\n",
    "#’relu’ activation function.\n",
    "\n",
    "starttime = time.time()\n",
    "regr = MLPRegressor(hidden_layer_sizes = (32), random_state=7, activation='relu', \n",
    "        learning_rate_init=0.01, max_iter=5000)\n",
    "# This the cross-validation. It is the important and expensive part of the code.\n",
    "scores = cross_val_score(regr,X_train_quant,y_train_scale)  \n",
    "# record the mean and std of the score\n",
    "score_mean = scores.mean()\n",
    "score_std = scores.std()\n",
    "endtime = time.time()\n",
    "looptimes1 = endtime - starttime\n",
    "\n",
    "print('layer size of 32 score =','{0:.4g}'.format(score_mean), 'Time taken for this case is', '{0:.4g}'.format(looptimes1))\n",
    "\n",
    "\n",
    "\n",
    "starttime = time.time()\n",
    "regr = MLPRegressor(hidden_layer_sizes = (32,32,32), random_state=7, activation='relu', \n",
    "        learning_rate_init=0.01, max_iter=5000)\n",
    "# This the cross-validation. It is the important and expensive part of the code.\n",
    "scores = cross_val_score(regr,X_train_quant,y_train_scale)  \n",
    "# record the mean and std of the score\n",
    "score_mean = scores.mean()\n",
    "score_std = scores.std()\n",
    "endtime = time.time()\n",
    "looptimes2 = endtime - starttime\n",
    "\n",
    "print('layer size of (32,32,32) score =','{0:.4g}'.format(score_mean), 'Time taken for this case is', '{0:.4g}'.format(looptimes2))\n",
    "\n",
    "\n",
    "\n",
    "starttime = time.time()\n",
    "regr = MLPRegressor(hidden_layer_sizes = (32,32,32,32,32), random_state=7, activation='relu', \n",
    "        learning_rate_init=0.01, max_iter=5000)\n",
    "# This the cross-validation. It is the important and expensive part of the code.\n",
    "scores = cross_val_score(regr,X_train_quant,y_train_scale)  \n",
    "# record the mean and std of the score\n",
    "score_mean = scores.mean()\n",
    "score_std = scores.std()\n",
    "endtime = time.time()\n",
    "looptimes3 = endtime - starttime\n",
    "looptimes = looptimes1 + looptimes2 + looptimes3\n",
    "print('layer size of (32,32,32,32,32) score =','{0:.4g}'.format(score_mean), 'Time taken for this case is', '{0:.4g}'.format(looptimes3))\n",
    "      \n",
    "print(\"Total training time = \", '{0:.4g}'.format(looptimes), \" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_______________________\n",
    "##### From the 4 previous test cases it seems logical to set the parameters as follows:\n",
    "\n",
    "Alpha = 0.0001, for our alpha test using 0.0001 offered the highest mean score and is just as fast as the test cases for 0.001 and 0.01, which offer slightly lower mean cross val scores.\n",
    "\n",
    "Activation Function = 'relu'. We notice how the relu activation function was typically faster than the logistic activation function with the mean scores being essentially the same, while testing two more values of layer size with relu.\n",
    "\n",
    "Using three layers with 32 neurons in each also performed the best regarding the hidden layer sizes parameter, while also having a moderately short training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing our final MLP Regressor after our hyper parameter search.\n",
    "\n",
    "\n",
    "starttime = time.time()\n",
    "regr = MLPRegressor(alpha = 0.0001, hidden_layer_sizes = (32,32,32), random_state=7, activation='relu', \n",
    "        learning_rate_init=0.01, max_iter=5000)\n",
    "# This the cross-validation. It is the important and expensive part of the code.\n",
    "scores = cross_val_score(regr,X_train_quant,y_train_scale)  \n",
    "# record the mean and std of the score\n",
    "score_mean = scores.mean()\n",
    "score_std = scores.std()\n",
    "endtime = time.time()\n",
    "looptimes = endtime - starttime\n",
    "\n",
    "print('Our score is','{0:.4g}'.format(score_mean), 'Time taken for this case is', '{0:.4g}'.format(looptimes))\n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Trainging and testing on the test data\n",
    "regr = MLPRegressor(alpha = 0.0001, hidden_layer_sizes = (32,32,32), random_state=7, activation='relu', \n",
    "        learning_rate_init=0.01, max_iter=5000)\n",
    "regr.fit(X_train_quant,y_train_scale)\n",
    "\n",
    "\n",
    "print(\"The final regressor score is\", '{0:.4g}'.format(regr.score(X_test_quant,y_test_scale)),\"\\n\")\n",
    "\n",
    "y_predict = regr.predict(X_test_quant)\n",
    "### Plotting predicted values using our MLP regressor and actual values for same X data to see correspondence \n",
    "plt.plot(y_test_scale[:50], '-o', label=\"y_true\")\n",
    "plt.plot(y_predict[:50],'-o', label=\"y_predict\")\n",
    "plt.xlabel(\"Example 0 to 50\", fontsize=14)\n",
    "plt.ylabel(\"target y\", fontsize=14)\n",
    "plt.title(\"Predicted vs True values of House Price\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(y_test_scale[1000:1050], '-o', label=\"y_true\")\n",
    "plt.plot(y_predict[1000:1050],'-o', label=\"y_predict\")\n",
    "plt.xlabel(\"Example 1000 to 1050\", fontsize=14)\n",
    "plt.ylabel(\"target y\", fontsize=14)\n",
    "plt.title(\"Predicted vs True values of House Price\")\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From our plots and regressor score we can see how we can make extremely good estimations on housing prices having successfully trained a model using a sample from the data and chosen hyperparameters to roughly 80% accuracy. Also given the size of the dataset it also is relatively fast. This could be useful for future building projects in which potential buyers or investors may want to know how lucrative a project may be to undertake. We could improve our scores by trying different train test splits and possibly performing grid searches for the best parameters, but this will be more time consuming and computer intensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Dependence Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['MedInc', 'AveOccup', 'HouseAge', 'AveRooms','AveBedrms', 'Population','Latitude','Longitude']\n",
    "## Need data as a dataframe here hence\n",
    "X_train_quant_df = pd.DataFrame(data = X_train_quant,index = None, columns = features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = make_pipeline(MLPRegressor(alpha = 0.01, hidden_layer_sizes = (32,32,32), random_state=7, activation='relu', \n",
    "        learning_rate_init=0.01, max_iter=5000))\n",
    "regr.fit(X_train_quant,y_train_scale)\n",
    "plot_partial_dependence(regr, X_train_quant_df, features,\n",
    "                        n_jobs=5, grid_resolution=20)\n",
    "\n",
    "fig = plt.gcf()\n",
    "fig.suptitle('Partial dependence of house value on non-location features\\n'\n",
    "             'for the California housing dataset, with MLPRegressor')\n",
    "fig.subplots_adjust(hspace=0.9)\n",
    "fig.set_size_inches(9, 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the partial dependence plots we note how house value increases quickly as income increases. We note how houses in less populated areas lead to more expensive houses, likely due to the fact rich areas of california are less densely populated, hence you pay a premium for more \"space\"\n",
    "We note how as we decrease longitude and latitude the cheaper houses get."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
